Kirjasampo
==========================
The project is developed in a public repo on the customer’s GitHub [account](https://github.com/libraries-fi/kirjasampo-api/tree/develop). 
# Setting up Vagrant
The setup seems to require quite specific versions of Vagrant and Ansible. It works somewhatwell withVirtualBox 5.1,Vagrant 2.1.4, Ansible 2.1.1.0 should work too according to the customerwho created the box.Also make sure that you are using Python 2.7 and jinja2 2.8.1
You can use the follow command to install it on Ubuntu:
```sh
$ wget https://releases.hashicorp.com/vagrant/2.1.4/vagrant_2.1.4_x86_64.deb
$ sudo dpkg -i vagrant_2.1.4_x86_64.deb
$ python2.7 -m pip install ansible==2.1.1
$ python2.7 -m pip install jinja2=2.8.1
```
1. Clone this [repo](https://github.com/libraries-fi/kirjasampo-api-vagrant)
2. Initialize submodules ```git submodule update --init```
3. Go to the kifi_playbooks directory and initialize submodules there too. The submodules arehosted in the customer’s Gitolite so they need your SSH key to give you access to them.
4. Run ```vagrant up``` in the root directory.
5. Checkout the develop branch of the kirjasampo-api repo that should now be located in theshared folder in the kirjasampo-api-vagrant repo.
6. Run ```vagrant ssh``` and the project files should be in ```/srv/httpd/api.kirjasampo.local```.
7. Install a database vagrant (e.g. MariaDB/MySQL) and create a user (e.g. api_platforms) anddatabase (e.g. api_platforms)8. Go to /srv/httpd/api.kirjasampo.local and run composer install and enter the database configsyou chose. Example:
```sh
database_host: localhost
database_port: 3306
database_name: api_platform
database_user: api_platform
database_password: api_platform
```
Make sure that the driver is correctly written in the configuration file ```app/config/config.yml```. For MariaDB/MySQL it should have the following meaning ```driver:   pdo_mysql```

8. Add ```api.kirjasampo.local``` to your hosts file. The IP should be ```192.168.33.24``` but you canconfirm it with the ip a command. If the server receives 500 error, a possible solution is to change the following value in the kernel ```fileweb/app.php```.
```$kernel = new AppKernel('dev', true);```

# Converting database dumps

## Splitting files into smaller pieces
The graph database dump (the .nq file) is converted using 3 symfony console commands
because the file is about 7.5 million lines and 1.5 GB. The first command splits the file into
smaller pieces. The file is first sorted alphabetically to have the first quads always be right below
each other. This is required because of how the n-quad to JSON-LD conversion works. The
process is described [here](https://dvcs.w3.org/hg/rdf/raw-file/default/rdf-json/index.html#section-serialization).

By default the size is 250,000 lines and after the 250,000 th line the command keeps going
forward until the first quad (subject) is different. This is done to get the whole graph into the
same file, which is required in the next step. The files generated by this command have a suffix
of 4, e.g. test-import.nq0000.
If you only have a huge dump of the SAHA database then you might want to only use a small
part of it for testing, otherwise the indexing will take very long:
```head -n 250000 kirjasampo.nq > kirjasampo-250000.nq```

Example command:
```php bin/console saha:dump:split /home/vagrant/test-import-1000000.nq```

## Converting n-quads into JSON-LD
The next step is to convert the n-quads to JSON-LD. This is done by the next command. The
conversion itself is done using EasyRdf. The library opens the first file (e.g. test-import.nq0000)
and looks for all the lines that have the same subject and turns that into a JSON-LD object.
Then it does it for the next lines with a different subject until all lines in that file are converted.
Then it continues to the next file with a suffix of 0001 and so on. This command outputs files
with the same name as the command above, except that the “nq” part is replaced with “json”,
e.g. test-import-100000.json0000.

This command also adds an extra line to the dump before every JSON-LD object to make the
data compatible with the Elasticsearch bulk API so that it can be indexed to ES in the next step.
The ES index and type are defined in the conversion command and for now they are just
“books” and “book”. This should probably be changed in the future.

Example command:
```php bin/console saha:dump:convert /home/vagrant/test-import-1000000.nq```

**Note**:
- You don’t have to use any file name suffix in the command
- You may need to bump up the memory available on the vagrant box (default 512MB) or
the process might get killed while executing

## Indexing to Elasticsearch
The last step is to index the data to Elasticsearch. This is a simple process because it’s just a
wrapper for a terminal command.
Example command:
```php bin/console saha:dump:index /home/vagrant/test-import-1000000.json```

# How the API works
API Platform is built on top of Symfony (currently v3.2) so it works the same way. There are two
bundles: ```SahaBundle``` for the database dump commands (although this could be part of the
AppBundle itself) and the ```AppBundle``` that has all the API code.
The initial plan was to map the ES documents to PHP entities but this would be quite
challenging because API Platform has no idea what kind of document it is dealing with (whether
it’s a book, an author etc.). So for now everything uses the same Document entity. The first or
second item in the dumps (subject or predicate) could perhaps be used to identify the document
type but there are quite a lot of different subjects and predicates (e.g. http://www.yso.fi/onto/kaunokki#luetteloija could be mapped to one entity).

There is one controller called ```DocumentController``` that currently handles all the incoming
requests to the API. There is a method for getting a single document, a collection and searching
for documents. Single documents can be searched by their ID, which is the ID generated by ES
when documents are indexed.
The ```DocumentController``` is quite simple, it just calls the item or collection data provider
depending on whether it has to get a single document or a collection. The two data providers
are used to communicate with Elasticsearch.
